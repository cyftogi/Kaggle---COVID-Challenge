{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.layers import GlobalMaxPooling2D, GlobalAveragePooling2D, Dense, Conv2D\n",
    "from keras.models import Model\n",
    "import os\n",
    "import matplotlib.image as mpimg \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(image_batch, label_batch, k=5):\n",
    "  plt.figure(figsize=(12,12))\n",
    "  for n in range(k**2):\n",
    "      ax = plt.subplot(k,k,n+1)\n",
    "      plt.imshow(image_batch[n])\n",
    "      plt.title(labels[label_batch[n]==1][0])\n",
    "      plt.axis('off')\n",
    "        \n",
    "def categorical_to_index(y):\n",
    "    # turn one-hot matrix back to single labels \n",
    "    return np.tile(np.arange(len(y[0])), (len(y),1))[y==1]\n",
    "\n",
    "\n",
    "def k_hold_data(image_batch, label_batch, cutoff = [0, 0.2, 0.4, 0.6, 0.8, 1]):\n",
    "    # k-fold\n",
    "    n = len(cutoff)-1 \n",
    "    Images = []; Labels = []\n",
    "    for i in range(1, len(cutoff)):\n",
    "        left = np.int(cutoff[i-1]*len(filenames))\n",
    "        right = np.int(cutoff[i]*len(filenames))\n",
    "        Images.append(image_batch[left:right+1])\n",
    "        Labels.append(label_batch[left:right+1])\n",
    "    Images = np.array(Images)\n",
    "    Labels = np.array(Labels)\n",
    "\n",
    "    Train_X = []; Train_Y = []; Val_X = []; Val_Y = []\n",
    "    for i in range(n):\n",
    "        idx = np.arange(n).tolist()\n",
    "        idx.remove(i)\n",
    "        Val_X.append(Images[i])\n",
    "        Val_Y.append(Labels[i])\n",
    "        Train_X.append(np.concatenate(Images[idx]))\n",
    "        Train_Y.append(np.concatenate(Labels[idx]))\n",
    "    return Train_X, Train_Y, Val_X, Val_Y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-NN classifiers Performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_path = \"/kaggle/input/4771-sp20-covid\"\n",
    "train_dir = input_path+\"/train/train\"\n",
    "\n",
    "# read y \n",
    "with open(input_path+\"/train.csv\", \"r\") as f:\n",
    "    train_y=pd.read_csv(f)[\"label\"]\n",
    "\n",
    "labels = np.unique(train_y)\n",
    "# labels\n",
    "\n",
    "mapping = {}\n",
    "for i in range(len(labels)):\n",
    "    mapping[labels[i]] = i\n",
    "    \n",
    "train_y = train_y.apply(lambda x: mapping[x])\n",
    "train_y = keras.utils.to_categorical(train_y)\n",
    "# train_y\n",
    "\n",
    "\n",
    "# read x\n",
    "filenames = os.listdir(train_dir)\n",
    "h, w = 200, 200  \n",
    "train_x = np.full((len(filenames),h,w,3),np.nan)\n",
    " \n",
    "for i in range(len(filenames)):\n",
    "    filename = filenames[i]\n",
    "    file = os.path.join(train_dir, filename)\n",
    "    pic = keras.preprocessing.image.load_img(file, grayscale=0, color_mode='rgb', target_size=(h,w))\n",
    "#     plt.imshow(pic)\n",
    "    \n",
    "    # note the images are not ordered\n",
    "    order = np.int(filename.split(\".\")[0].split(\"-\")[-1])\n",
    "    train_x[order] = np.array(pic)\n",
    "    \n",
    "    \n",
    "# image generator \n",
    "image_generator = keras.preprocessing.image.ImageDataGenerator(rescale=1/255)   # will do validation_split in model.fit\n",
    "train_data_generator = image_generator.flow(train_x, train_y, batch_size=len(filenames))\n",
    "# image_batch, label_batch = next(train_data_generator)\n",
    "# image_batch.shape\n",
    "# # data, h, w, channel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(train_data_generator)\n",
    "image_batch = np.array([np.ravel(image_batch[i]) for i in range(len(image_batch))])\n",
    "label_batch = categorical_to_index(label_batch)\n",
    "Train_X, Train_Y, Val_X, Val_Y  = k_hold_data(image_batch, label_batch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try Naive Bayes classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "print(\"Naive Bayes accuracy\")\n",
    "NB_accu = []\n",
    "for i in range(len(Train_X)):\n",
    "    train_x, train_y, val_x, val_y = Train_X[i], Train_Y[i], Val_X[i], Val_Y[i]\n",
    "    val_y_pred = gnb.fit(train_x, train_y).predict(val_x)\n",
    "    NB_accu.append(np.sum(val_y==val_y_pred)/len(val_y))\n",
    "print(np.mean(NB_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try SVM\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='rbf')\n",
    "print(\"SVM accuracy\")\n",
    "SVM_accu = []\n",
    "for i in range(len(Train_X)):\n",
    "    train_x, train_y, val_x, val_y = Train_X[i], Train_Y[i], Val_X[i], Val_Y[i]\n",
    "    clf.fit(train_x, train_y)\n",
    "    val_y_pred = clf.predict(val_x)\n",
    "    SVM_accu.append(np.sum(val_y==val_y_pred)/len(val_y))\n",
    "print(np.mean(SVM_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try KNN classifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "KNN_accu = []\n",
    "print(\"KNN accuracy\")\n",
    "k=[3,5,7,10,15,25,50,75,100,125,150,175,200,250,300,350,400]\n",
    "for n in k:\n",
    "    accu = []\n",
    "    for i in range(len(Train_X)):\n",
    "        train_x, train_y, val_x, val_y = Train_X[i], Train_Y[i], Val_X[i], Val_Y[i]\n",
    "        neigh = KNeighborsClassifier(n_neighbors=n, algorithm='kd_tree')\n",
    "        neigh.fit(train_x, train_y)\n",
    "        val_y_pred = neigh.predict(val_x)\n",
    "        accu.append(np.sum(val_y==val_y_pred)/len(val_y))\n",
    "    KNN_accu.append(np.mean(accu))\n",
    "    print(KNN_accu[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(k, KNN_accu)\n",
    "plt.title(\"KNN accuracy\")\n",
    "plt.xlabel(\"n_neigh\")\n",
    "plt.ylabel(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model Performance  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"/kaggle/input/4771-sp20-covid\"\n",
    "train_dir = input_path+\"/train/train\"\n",
    "\n",
    "# read y \n",
    "with open(input_path+\"/train.csv\", \"r\") as f:\n",
    "    train_y=pd.read_csv(f)[\"label\"]\n",
    "\n",
    "labels = np.unique(train_y)\n",
    "# labels\n",
    "\n",
    "mapping = {}\n",
    "for i in range(len(labels)):\n",
    "    mapping[labels[i]] = i\n",
    "    \n",
    "train_y = train_y.apply(lambda x: mapping[x])\n",
    "train_y = keras.utils.to_categorical(train_y)\n",
    "# train_y\n",
    "\n",
    "\n",
    "# read x\n",
    "filenames = os.listdir(train_dir)\n",
    "h, w = 200, 200  \n",
    "train_x = np.full((len(filenames),h,w,3),np.nan)\n",
    " \n",
    "for i in range(len(filenames)):\n",
    "    filename = filenames[i]\n",
    "    file = os.path.join(train_dir, filename)\n",
    "    pic = keras.preprocessing.image.load_img(file, grayscale=0, color_mode='rgb', target_size=(h,w))\n",
    "#     plt.imshow(pic)\n",
    "    \n",
    "    # note the images are not ordered\n",
    "    order = np.int(filename.split(\".\")[0].split(\"-\")[-1])\n",
    "    train_x[order] = np.array(pic)\n",
    "    \n",
    "    \n",
    "# image generator \n",
    "permu = np.random.permutation(len(filenames))\n",
    "cutoff = np.int(0.8*len(filenames))\n",
    "image_generator = keras.preprocessing.image.ImageDataGenerator(1/255)   \n",
    "train_data_generator = image_generator.flow(train_x[permu][:cutoff], train_y[permu][:cutoff], batch_size=len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(train_data_generator)\n",
    "show_batch(image_batch, label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "VGG19_MODEL = keras.applications.VGG19(include_top=False, weights='imagenet', classes=len(labels))\n",
    "for l in VGG19_MODEL.layers:\n",
    "    l.trainable = False   # use pretrained weights  \n",
    "\n",
    "\n",
    "y = VGG19_MODEL.output\n",
    "\n",
    "y = GlobalMaxPooling2D()(y)\n",
    "\n",
    "y = Dense(256, activation='relu')(y)\n",
    "\n",
    "y = Dense(128, activation='relu')(y)\n",
    "\n",
    "y = Dense(56, activation='relu')(y)\n",
    "\n",
    "y = Dense(len(labels), activation='softmax')(y)\n",
    "\n",
    "model = Model(input=VGG19_MODEL.input, output=y)\n",
    "\n",
    "model.compile(optimizer='RMSprop', loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(train_data_generator)\n",
    "# print(image_batch.shape)\n",
    "model.fit(image_batch, label_batch, epochs=200, batch_size=64, validation_split=0.1, shuffle=True)\n",
    "# 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error analysis \n",
    "val_x, val_y = train_x[permu][cutoff:], train_y[permu][cutoff:]\n",
    "val_y = categorical_to_index(val_y)\n",
    "val_y_pred = np.argmax(model.predict(val_x), axis=1)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "method = None\n",
    "precision = precision_score(val_y, val_y_pred, average=method)\n",
    "recall = recall_score(val_y, val_y_pred, average=method)\n",
    "f1 = f1_score(val_y, val_y_pred, average=method)\n",
    "\n",
    "df = pd.DataFrame(index = ['bacterial', 'covid', 'normal', 'viral'])\n",
    "df[\"precision\"] = precision\n",
    "df[\"recall\"] = recall\n",
    "df[\"f1 score\"] = f1\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "c = confusion_matrix(val_y, val_y_pred)\n",
    "df = pd.DataFrame(c, index = ['bacterial', 'covid', 'normal', 'viral'], columns=['bacterial', 'covid', 'normal', 'viral'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If not preprocessing images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if not rescaling the images \n",
    "\n",
    "# with open(input_path+\"/train.csv\", \"r\") as f:\n",
    "#     train_y=pd.read_csv(f)[\"label\"]\n",
    "\n",
    "# labels = np.unique(train_y)\n",
    "\n",
    "# mapping = {}\n",
    "# for i in range(len(labels)):\n",
    "#     mapping[labels[i]] = i\n",
    "    \n",
    "# train_y = train_y.apply(lambda x: mapping[x])\n",
    "# train_y = keras.utils.to_categorical(train_y)\n",
    "\n",
    "\n",
    "\n",
    "# filenames = os.listdir(train_dir)\n",
    "# h, w = 200, 200  \n",
    "# train_x = np.full((len(filenames),h,w,3),np.nan)\n",
    " \n",
    "# for i in range(len(filenames)):\n",
    "#     filename = filenames[i]\n",
    "#     file = os.path.join(train_dir, filename)\n",
    "#     pic = keras.preprocessing.image.load_img(file, grayscale=0, color_mode='rgb', target_size=(h,w))\n",
    "#     order = np.int(filename.split(\".\")[0].split(\"-\")[-1])\n",
    "#     train_x[order] = np.array(pic)\n",
    "    \n",
    "    \n",
    "    \n",
    "# permu = np.random.permutation(len(filenames))\n",
    "# cutoff = np.int(0.8*len(filenames))\n",
    "# image_generator = keras.preprocessing.image.ImageDataGenerator()   # no rescaling \n",
    "# train_data_generator = image_generator.flow(train_x[permu][:cutoff], train_y[permu][:cutoff], batch_size=len(filenames))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# VGG19_MODEL = keras.applications.VGG19(include_top=False, weights='imagenet', classes=len(labels))\n",
    "# for l in VGG19_MODEL.layers:\n",
    "#     l.trainable = False   \n",
    "\n",
    "# y = VGG19_MODEL.output\n",
    "# y = GlobalMaxPooling2D()(y)\n",
    "# y = Dense(256, activation='relu')(y)\n",
    "# y = Dense(128, activation='relu')(y)\n",
    "# y = Dense(56, activation='relu')(y)\n",
    "# y = Dense(len(labels), activation='softmax')(y)\n",
    "# model = Model(input=VGG19_MODEL.input, output=y)\n",
    "# model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# image_batch, label_batch = next(train_data_generator)\n",
    "# model.fit(image_batch, label_batch, epochs=200, batch_size=64, validation_split=0.2, shuffle=True)\n",
    "\n",
    "# val_x, val_y = train_x[cutoff:], train_y[cutoff:]\n",
    "# val_y = categorical_to_index(val_y)\n",
    "# val_y_pred = np.argmax(model.predict(val_x), axis=1)\n",
    "# print('accuracy:', np.sum(val_y==val_y_pred)/len(val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if not resizing the images \n",
    "\n",
    "# input_path = \"/kaggle/input/4771-sp20-covid\"\n",
    "# train_dir = input_path+\"/train/train\"\n",
    "\n",
    "# with open(input_path+\"/train.csv\", \"r\") as f:\n",
    "#     train_y=pd.read_csv(f)[\"label\"]\n",
    "\n",
    "# labels = np.unique(train_y)\n",
    "\n",
    "# mapping = {}\n",
    "# for i in range(len(labels)):\n",
    "#     mapping[labels[i]] = i\n",
    "    \n",
    "# train_y = train_y.apply(lambda x: mapping[x])\n",
    "# train_y = keras.utils.to_categorical(train_y)\n",
    "\n",
    "\n",
    "\n",
    "# filenames = os.listdir(train_dir)\n",
    "# h, w = 600,600\n",
    "# train_x = [[] for i in range(len(filenames))]\n",
    " \n",
    "# for i in range(len(filenames)):\n",
    "#     filename = filenames[i]\n",
    "#     file = os.path.join(train_dir, filename)\n",
    "#     pic = keras.preprocessing.image.load_img(file, grayscale=0, color_mode='rgb', target_size=(h,w))\n",
    "#     order = np.int(filename.split(\".\")[0].split(\"-\")[-1])\n",
    "#     train_x[order] = np.array(pic)\n",
    "# train_x = np.array(train_x)    \n",
    "    \n",
    "\n",
    "# permu = np.random.permutation(len(filenames))\n",
    "# cutoff = np.int(0.8*len(filenames))\n",
    "# image_generator = keras.preprocessing.image.ImageDataGenerator(1/255)   \n",
    "# train_data_generator = image_generator.flow(train_x[permu][:cutoff], train_y[permu][:cutoff], batch_size=len(filenames))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# VGG19_MODEL = keras.applications.VGG19(include_top=False, weights='imagenet', classes=len(labels))\n",
    "# for l in VGG19_MODEL.layers:\n",
    "#     l.trainable = False   \n",
    "\n",
    "# y = VGG19_MODEL.output\n",
    "# y = GlobalMaxPooling2D()(y)\n",
    "# y = Dense(256, activation='relu')(y)\n",
    "# y = Dense(128, activation='relu')(y)\n",
    "# y = Dense(56, activation='relu')(y)\n",
    "# y = Dense(len(labels), activation='softmax')(y)\n",
    "# model = Model(input=VGG19_MODEL.input, output=y)\n",
    "# model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# image_batch, label_batch = next(train_data_generator)\n",
    "# model.fit(image_batch, label_batch, epochs=200, batch_size=64, validation_split=0.2, shuffle=True)\n",
    "\n",
    "# val_x, val_y = train_x[cutoff:], train_y[cutoff:]\n",
    "# val_y = categorical_to_index(val_y)\n",
    "# val_y_pred = np.argmax(model.predict(val_x), axis=1)\n",
    "# print('accuracy:', np.sum(val_y==val_y_pred)/len(val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_dir = input_path+\"/test/test\"\n",
    "filenames = os.listdir(test_dir)\n",
    "test_x = np.full((len(filenames),h,w,3),np.nan)\n",
    " \n",
    "for i in range(len(filenames)):\n",
    "    filename = filenames[i]\n",
    "    file = os.path.join(test_dir, filename)\n",
    "    pic = keras.preprocessing.image.load_img(file, grayscale=0, color_mode='rgb', target_size=(h,w))\n",
    "    order = np.int(filename.split(\".\")[0].split(\"-\")[-1])\n",
    "    test_x[order] = np.array(pic)\n",
    "test_data_generator = image_generator.flow(test_x, y=None, batch_size=len(filenames), shuffle=False)\n",
    "image_batch = next(test_data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict(image_batch)\n",
    "pred = np.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_mapping = {}\n",
    "for i in range(len(labels)):\n",
    "    inverse_mapping[i] = labels[i]\n",
    "\n",
    "y = pd.DataFrame(columns=['Id','label'])\n",
    "y['Id'] = np.arange(len(pred))\n",
    "for i in range(len(pred)):\n",
    "    y.iloc[i,1] = inverse_mapping[pred[i]]\n",
    "y.to_csv(\"submission.csv\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
